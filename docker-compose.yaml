# Docker Compose file
version: "3.9"
x-airflow-common:
  &airflow-common
  # image: apache/airflow:2.9.2
  build: 
    context: ./airflow
    dockerfile : ./Dockerfile
    args:
      AIRFLOW_IMAGE_NAME: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.2}
  image: airflow-custom:2.9.2
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
  volumes:
    # - ./.env:/opt/airflow/env
    # - ./airflow/dags:/opt/airflow/dags
    # - ./airflow/logs:/opt/airflow/logs
    # - ./airflow/config:/opt/airflow/config
    # - ./airflow/requirements.txt:/opt/airflow/requirements.txt
    # - ./airflow/plugins:/opt/airflow/plugins
    # - ./airflow/include:/opt/airflow/include
    # - /etc/localtime:/etc/localtime:ro
    - ${AIRFLOW_PROJ_DIR:-.}:/opt/airflow/data
    - ./airflow:/opt/airflow
    - /etc/localtime:/etc/localtime:ro
    - ./.env:/opt/airflow/.env
    - ${AIRFLOW_PROJ_DIR:-.}/airflow.cfg:/opt/airflow/airflow.cfg

  user: "${AIRFLOW_UID:-50000}:0"
  # user: "0:0"



  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy
services:
  # HDFS & YARN 
  namenode:
    container_name: namenode
    image: apache/hadoop:3
    hostname: namenode
    command: bash -c "if [ ! -f /tmp/hadoop-root/dfs/name/.formatted ]; then hdfs namenode -format && touch /tmp/hadoop-root/dfs/name/.formatted; fi && hdfs namenode"
    ports:
      - 9870:9870
      - 8020:8020
      - 9000:9000
    user: root
    env_file:
      - ./hadoop.env
    volumes:
      - hdfs_namenode:/tmp/hadoop-root/dfs/name
    networks:
      - spotify_network

  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    hostname: datanode 
    command: ["hdfs", "datanode"]
    ports:
      - 9864:9864
      - 9866:9866
    expose:
      - 50010
    env_file:
      - ./hadoop.env
    user: root
    volumes:
      - hadoop_datanode:/tmp/hadoop-root/dfs/data
    networks:
      - spotify_network

  resourcemanager:
    image: apache/hadoop:3.3.6
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
      - 8088:8088
    env_file:
      - ./hadoop.env
    volumes:
      - ./test.sh:/opt/test.sh
    user: root
    networks:
      - spotify_network

  nodemanager:
    image: apache/hadoop:3.3.6
    command: ["yarn", "nodemanager"]
    env_file:
      - ./hadoop.env
    ports:
      - 8042:8042
    user: root
    networks:
      - spotify_network
  #MINIO
  # minio1:
  #   container_name: "minio1"
  #   image: "minio/minio"
  #   ports:
  #     - "9001:9001"
  #     - "9000:9000"
  #   command: ["server", "/data", "--console-address", ":9001"]
  #   volumes:
  #     - ./minio:/data
  #   env_file:
  #     - .env
  #   networks:
  #     - spotify_network
  

  # mc1:
  #   image: minio/mc
  #   environment:
  #     - AWS_ACCESS_KEY_ID=minio
  #     - AWS_SECRET_ACCESS_KEY=minio123
  #     - AWS_REGION=us-east-1
  #   entrypoint: >-
  #     /bin/sh -c " until (/usr/bin/mc config host add minio http://minio1:9000 minio minio123) do echo '...waiting...' && sleep 1; done; /usr/bin/mc mb minio/datalake ; /usr/bin/mc policy set public minio/datalake; exit 0; "
  #   depends_on:
  #     - minio1
  #   networks:
  #     - spotify_network

  # Spark
  # spark-master1:
  #   build:
  #     context: ./spark
  #     dockerfile: ./Dockerfile
  #   image: spark_master:latest
  #   container_name: "spark-master1"
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_LOCAL_IP=spark-master1
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   ports:
  #     - "7077:7077"
  #     - "8080:8080"
  #   expose:
  #     - "7077"
  #   volumes:
  #     - ./airflow/include:/usr/local/airflow/include
  #     - ./spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
  #   networks:
  #     - spotify_network
    
  # spark_worker:
  #   image: docker.io/bitnami/spark:3.4.3
  #   deploy:
  #     replicas: 2
  #   command: start-slave.sh spark://spark-master1:7077
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master1:7077
  #     - SPARK_WORKER_MEMORY=2G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   networks:
  #     - spotify_network
  #   depends_on:
  #     - spark-master1

  notebook:
    build:
      context: ./notebooks
      dockerfile: ./Dockerfile
    user : root
    container_name: spark_notebook1
    environment:
      - JUPYTER_ENABLE_LAB="yes"
      - GRANT_SUDO="yes"
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./notebooks/spark-defaults.conf:/usr/local/spark/conf/spark-defaults.conf
    ports:
      - "8888:8888"
      - "4040:4040"
    networks:
      - spotify_network

  # Airflow

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        mkdir -p /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins}

        which airflow
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    user: "0:0"
    env_file:
      - .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      # - .:/sources
    networks:
      - spotify_network
  # airflow-cli:
  #   <<: *airflow-common
  #   profiles:
  #     - debug
  #   environment:
  #     <<: *airflow-common-env
  #     CONNECTION_CHECK_MAX_COUNT: "0"
  #   # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
  #   command:
  #     - bash
  #     - -c
  #     - airflow
  #   networks:
  #     - spotify_network

    
  # de_mysql1:
  #   image: mysql:8.0
  #   container_name: "de_mysql1"
  #   ports:
  #     - "3307:3306"
  #   volumes:
  #     - ./dataset:/tmp/dataset
  #     - ./mysql:/var/lib/mysql
  #   env_file:
  #     - .env
  #   networks:
  #     - de_network
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - spotify_network
  # de_psql2:
  #   image: postgres:15
  #   container_name: "de_psql2"
  #   ports:
  #     - "5433:5432"
  #   volumes:
  #     - ./postgresql/data:/var/lib/postgresql/data
  #   env_file:
  #     - .env
  #   networks:
  #     - de_network

  # redis:
  #   image: redis:latest
  #   container_name: redis
  #   ports:
  #     - "6379:6379"
  #   networks:
  #     - spotify_network

  airflow-webserver:
    <<: *airflow-common
    command: webserver --workers 2
    ports:
      - "8085:8080"
    env_file:
      - .env
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8085/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          memory: 512M
    networks:
      - spotify_network

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    env_file:
      - .env
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - spotify_network

  trino:
    image: trinodb/trino
    container_name: trino
    build  :
      context : .
      dockerfile: Dockerfile
    ports:
      - "8095:8080"
    volumes:
      - ./trino/etc:/opt/trino/etc
      - ./trino/catalog:/opt/trino/etc/catalog
      - ./hadoop-config/core-site.xml:/etc/hadoop/conf/core-site.xml

    environment:
      HADOOP_CONF_DIR: /etc/hadoop/conf
    depends_on:
      - hive-metastore-db
    networks:
      - spotify_network

  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    ports:
      - "3000:3000"
    environment:
      - MB_DB_FILE=/metabase.db
      - MB_LOCALE=en
    volumes:
      - ./metabase:/metabase.db
    networks:
      - spotify_network
  
  hive-metastore-db:
    image: postgres:13
    container_name: hive-metastore-db
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
    ports:
      - "5436:5432"
    volumes:
      - ./hive/metastore:/var/lib/postgresql/data
    networks:
      - spotify_network

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    environment:
      HIVE_METASTORE_DRIVER: org.postgresql.Driver
      HIVE_METASTORE_JDBC_URL: jdbc:postgresql://hive-metastore-db:5432/metastore
      HIVE_METASTORE_DATABASE_TYPE: postgres
      HIVE_METASTORE_DATABASE_HOST: hive-metastore-db
      HIVE_METASTORE_DATABASE_NAME: metastore
      HIVE_METASTORE_DATABASE_USER: hive
      HIVE_METASTORE_DATABASE_PASSWORD: hive
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      HIVE_METASTORE_WAREHOUSE_DIR: hdfs://namenode:8020/warehouse_layer
    command: /opt/hive/bin/hive --service metastore
    volumes:
      - ./hadoop-config/core-site.xml:/etc/hadoop/core-site.xml
      - ./hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
    ports:
      - "9083:9083"
      - "10000:10000"
    depends_on:
      - hive-metastore-db
      - namenode
    networks:
      - spotify_network




volumes : 
  postgres-db-volume:
  hadoop_datanode:
  hdfs_namenode:

networks:
  spotify_network:
    driver: bridge
    name: spotify_network
